# Building DeepAgent Harnesses for Terminal Bench 2.0 with Harbor

Build, evaluate, and improve custom agents on Terminal Bench 2.0 with **DeepAgents** (LangChain's agent framework), **Harbor** (a framework for running agents in container environments), and **LangSmith** (built in logging and observability).

## What is This?

This repo provides a **DeepAgent harness** - a complete agent implementation built on [LangChain DeepAgents](https://github.com/langchain-ai/deepagents) that can:

- **Run Harbor tasks** - Execute any Harbor benchmark in sandboxed environments
- **Customize your agent harnesses** - Modify prompts, tools, and behavior for your use case
- **Log to LangSmith** - Automatically trace all runs with full observability
- **Measure & improve** - Analyze traces to optimize your agent harness design


## Supported Sandbox Providers
Run your agent in multiple execution environments:
- ğŸ³ **Docker** (local)
- â˜ï¸ **Modal, Daytona, E2B** - Cloud sandboxes

## Quick Start

### 1. Installation

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
git clone https://github.com/vtrivedy/harbor-deepagents
cd harbor-deepagents
uv python install 3.12
uv sync && uv pip install -e .
uv tool install harbor
```

Editable installs are the default, so your changes under `src/` are live as soon
as you save them. Whenever you rebuild `.venv` (e.g., re-running `uv sync` after lock changes),
repeat `uv sync && uv pip install -e .` to reinstall the package before running a task. Run
`uv build` if you need a wheel/tarball for distribution.

### 2. Configuration

Create a `.env` file with your API keys:

```bash
# Required: Model provider API key
OPENAI_API_KEY=sk-proj-...                     
# OR
ANTHROPIC_API_KEY=sk-ant-...                   

# Optional: LangSmith tracing (recommended)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_pt_...
LANGCHAIN_PROJECT=...

# Optional: Cloud sandbox providers
# NOTE: Sandbox environments may have issues with downloading uv, python, container definition
DAYTONA_API_KEY=dtn_...                        
modal setup                                    
```

### 3. Run Your First Sample Task

```bash
# Run the web-scraper demo task (configs/job.yaml)
uv run harness --model openai/gpt-5-mini

# Or rely on MODEL_NAME from .env
uv run harness

# View Harbor results
cat jobs/<auto-job-name>/result.json | jq '.reward_stats.mean'
```

### 4. Run Terminal Bench 2.0

```bash
# Single task (Docker local)
uv run tb-docker --task prove-plus-comm --model openai/gpt-5-mini

# Full benchmark suite
uv run tb-docker --model openai/gpt-5-mini

# Daytona cloud sandbox (requires DAYTONA_API_KEY)
uv run tb-daytona --task prove-plus-comm --model anthropic/claude-sonnet-4-5-20250620
```

Each run creates a unique job folder: `<config>-<task>-<model>-<timestamp>`. Override with `--job-name my-run`.


### 5. View Traces in LangSmith

Enable tracing in `.env`:
```bash
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=lsv2_pt_...
LANGCHAIN_PROJECT=your-project-name
```

View traces at https://smith.langchain.com - Harbor reward scores are automatically logged as feedback.

## How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Harbor Orchestrator                        â”‚
â”‚   (Manages task execution across sandbox providers)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DeepAgent Harness                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LangChain 1.0 + DeepAgents Framework                â”‚ â”‚â”€â”€â”€â”€â”€â–¶ LangSmith
â”‚  â”‚ â€¢ Planning (write_todos)                            â”‚ â”‚       (Traces +
â”‚  â”‚ â€¢ Real Filesystem (FilesystemBackend)               â”‚ â”‚        Feedback)
â”‚  â”‚ â€¢ Subagents (task tool)                             â”‚ â”‚
â”‚  â”‚ â€¢ Custom Tools (bash from Harbor)                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Sandbox Environment (choose one)               â”‚
â”‚  â€¢ Docker (local) â€¢ Modal (cloud) â€¢ Daytona â€¢ E2B        â”‚
â”‚                                                           â”‚
â”‚  Provides: /app working directory + bash execution       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚  Task Tests   â”‚
                   â”‚  (Verifier)   â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                   Reward: 0.0 - 1.0
                   (logged to LangSmith)
```


## Customizing Your Agent

**System Prompt:** Edit [src/harbor_deepagents/agents/prompts.py](src/harbor_deepagents/agents/prompts.py).

**Model:** Update `model_name` inside [configs/job.yaml](configs/job.yaml) (or any Harbor config).
- OpenAI: `openai/gpt-5-mini`, `openai/gpt-4o`
- Anthropic: `anthropic/claude-sonnet-4-5-20250929`

**Drop-in custom agent:**
1. Start from [src/harbor_deepagents/agents/custom_agent.py](src/harbor_deepagents/agents/custom_agent.py). It subclasses `DeepAgentHarbor` but lets you swap prompts or override methods without touching the base harness.
2. Point your Harbor config at `harbor_deepagents.agents.custom_agent:CustomDeepAgent`. The stock [configs/job.yaml](configs/job.yaml) includes comments showing where to flip the `import_path`.
3. Iterate entirely in-source: run `uv run harness --config configs/job.yaml --dry-run` to verify Harbor resolves the agent, then drop `--dry-run` to execute tasks. LangSmith tracing keeps working so long as your `.env` provides the usual keys.

## Creating Custom Tasks

```bash
# Copy template
cp -r tasks/web-scraper-task tasks/my-custom-task

# Edit instruction.md and tests/test_solution.py
# Update configs/job.yaml with task path

# Run
uv run harness
```

## Learn More
- **Harbor**: https://github.com/HarborBench/harbor - Agent benchmarking framework
- **DeepAgents**: https://github.com/langchain-ai/deepagents - LangChain agent harness
- **LangSmith**: https://docs.smith.langchain.com - LLM observability platform
- **Terminal Bench 2.0**: Standard benchmark suite for coding agents
